{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,LSTM, Conv1D, Activation, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import keras\n",
    "\n",
    "\n",
    "from sz_utils import data_handler\n",
    "import pandas as pd\n",
    "\n",
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# collect the data\n",
    "preictal, interictal = data_handler.make_patient_windows(\"chb01\")\n",
    "\n",
    "# make the labels\n",
    "X = np.concatenate((preictal, interictal), axis=0)\n",
    "y = np.concatenate((np.ones((preictal.shape[0], 1)), np.zeros((interictal.shape[0], 1))), axis=0)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffle_indices = np.random.permutation(np.arange(X.shape[0]))\n",
    "X = X[shuffle_indices]\n",
    "y = y[shuffle_indices]\n",
    "\n",
    "# Split the data into train and test\n",
    "train_size = int(X.shape[0] * 0.8)\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "# Split test data into validation and test\n",
    "val_size = int(X_test.shape[0] * 0.5)\n",
    "X_val = X_test[:val_size]\n",
    "y_val = y_test[:val_size]\n",
    "X_test = X_test[val_size:]\n",
    "y_test = y_test[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (672, 1280, 22) y_train shape: (672, 1) X_val shape: (84, 1280, 22) y_val shape: (84, 1) X_test shape: (84, 1280, 22) y_test shape: (84, 1)\n"
     ]
    }
   ],
   "source": [
    "# shapes\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape, \"X_val shape:\", X_val.shape, \"y_val shape:\", y_val.shape, \"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2 \n",
    "input_shape_dataset = (X_train.shape[1], X_train.shape[2])\n",
    "input_shape_dataset\n",
    "\n",
    "def create_model_cnn_basic_1_layer(\n",
    "    input_shape_dataset: tuple = input_shape_dataset,\n",
    "    num_classes: int = num_classes,\n",
    "    debug: bool = False,\n",
    "    filters: int = 256,\n",
    "    kernel_size: int = 3,\n",
    "    pool_size: int = 2,\n",
    "    dropout: float = 0.1,\n",
    "    dense_size: int = 64,\n",
    "    loss: str = \"binary_crossentropy\",\n",
    "    optimizer: str = \"adam\",\n",
    "    metrics: list = [\"accuracy\"],\n",
    "\n",
    ") -> tf.keras.Model:\n",
    "\n",
    "    \"\"\"This function creates a basic convolutional neural network model with 2 convolutional layers, 2 dense layers and a softmax layer\n",
    "\n",
    "    :param input_shape_dataset: shape of the input data\n",
    "    :type input_shape_dataset: tuple\n",
    "    :param num_classes: number of classes\n",
    "    :type num_classes: int\n",
    "    :return: return a model\n",
    "    :rtype: tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    if debug:\n",
    "        print(\"------------model summary---------------\")\n",
    "        print(\"input_shape_dataset\", input_shape_dataset)\n",
    "        print(\"num_classes\", num_classes)\n",
    "\n",
    "    input_shape_dataset: tuple\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters, kernel_size, input_shape=(input_shape_dataset)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_size))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 1280, 22)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ns = (X, y)\n",
    "dataset_ns[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    experiment_name: str\n",
    "    model_name: str\n",
    "    model: tf.keras.Model\n",
    "    dataset: tuple\n",
    "    hyperparameters: dict\n",
    "    metrics: dict\n",
    "\n",
    "    def __init__(self, experiment_name, model_name, model, dataset, hyperparameters, metrics, num_folds=5):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.metrics = metrics\n",
    "        self.num_folds = num_folds\n",
    "        self.fold_no = 1\n",
    "        self.acc_per_fold = []\n",
    "        self.loss_per_fold = []\n",
    "        self.kfold = KFold(n_splits=self.num_folds, shuffle=True)\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "\n",
    "    def fit_evaluate(self):\n",
    "        try:\n",
    "            mlflow.tensorflow.autolog()\n",
    "            for i, (train, test) in enumerate(self.kfold.split(self.dataset[0], self.dataset[1])):\n",
    "                with mlflow.start_run(run_name=f\"fold_{i}\", nested=True):\n",
    "                    self.train = train\n",
    "                    self.test = test\n",
    "                    history = self.model.fit(self.dataset[0][train], self.dataset[1][train], epochs=self.hyperparameters[\"epochs\"], verbose=1)\n",
    "                    scores = self.model.evaluate(self.dataset[0][test], self.dataset[1][test], verbose=0)\n",
    "                    print(f'Score for fold {self.fold_no}: {self.model.metrics_names[0]} of {scores[0]}; {self.model.metrics_names[1]} of {scores[1]*100}%')\n",
    "                    self.acc_per_fold.append(scores[1] * 100)\n",
    "                    mlflow.log_param(\"acc_per_fold\", self.acc_per_fold)\n",
    "                    self.loss_per_fold.append(scores[0])\n",
    "                    self.log_metrics(history)\n",
    "                    self.fold_no += 1\n",
    "                # mlflow.end_run()\n",
    "\n",
    "            mlflow.log_artifact(self._log_graphs(history), artifact_path='Artifacts')\n",
    "\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            print(\"El error se produce en el fit, el error es: \", e)\n",
    "\n",
    "    def set_experiment(self):\n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "\n",
    "    def log_params(self):\n",
    "        mlflow.log_param(\"model_name\", self.model_name)\n",
    "        for key, value in self.hyperparameters.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "    def log_metrics(self, history):\n",
    "        for metric_name, metric_values in history.history.items():\n",
    "            for epoch, value in enumerate(metric_values):\n",
    "                mlflow.log_metric(f\"{metric_name}\", value, step=epoch)\n",
    "\n",
    "    def _log_graphs(self, history):\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "            \n",
    "            # Accuracy plot\n",
    "            ax[0].plot(history.history['accuracy'], label='train')\n",
    "            ax[0].plot(history.history['val_accuracy'], label='val')\n",
    "            ax[0].set_title('Accuracy')\n",
    "            ax[0].set_xlabel('Epoch')\n",
    "            ax[0].set_ylabel('Accuracy')\n",
    "            ax[0].legend()\n",
    "        \n",
    "            # Loss plot\n",
    "            ax[1].plot(history.history['loss'], label='train')\n",
    "            ax[1].plot(history.history['val_loss'], label='val')\n",
    "            ax[1].set_title('Loss')\n",
    "            ax[1].set_xlabel('Epoch')\n",
    "            ax[1].set_ylabel('Loss')\n",
    "            ax[1].legend()\n",
    "            \n",
    "            # Save the plots to a file\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('graphs.png')\n",
    "            return 'graphs.png'\n",
    "            \n",
    "    def log_artifacts(self):\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        self.set_experiment()\n",
    "        self.log_params()\n",
    "        history = self.fit_evaluate()\n",
    "        artifact_path = self._log_graphs(history)\n",
    "        mlflow.log_artifact(artifact_path, artifact_path='Artifacts')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing and refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-2.2.2-py3-none-any.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting docker<7,>=4.0.0\n",
      "  Using cached docker-6.0.1-py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (3.4.1)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (1.1.2)\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Downloading databricks-cli-0.17.6.tar.gz (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting querystring-parser<2\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (6.0)\n",
      "Requirement already satisfied: numpy<2 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (1.23.3)\n",
      "Requirement already satisfied: pyarrow<12,>=4.0.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (9.0.0)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Using cached sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: pandas<3 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (1.4.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (4.12.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (3.1.2)\n",
      "Requirement already satisfied: scipy<2 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (1.9.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (2.28.1)\n",
      "Requirement already satisfied: pytz<2023 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (2022.2.1)\n",
      "Requirement already satisfied: gitpython<4,>=2.1.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (3.1.27)\n",
      "Collecting sqlalchemy<3,>=1.4.0\n",
      "  Downloading SQLAlchemy-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gunicorn<21\n",
      "  Using cached gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Collecting cloudpickle<3\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: entrypoints<1 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (0.4)\n",
      "Collecting shap<1,>=0.40\n",
      "  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9,>=7.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (8.1.3)\n",
      "Collecting Flask<3\n",
      "  Downloading Flask-2.2.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<5,>=3.12.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (3.19.6)\n",
      "Collecting alembic<2\n",
      "  Downloading alembic-1.10.2-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib<4 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (3.6.0)\n",
      "Requirement already satisfied: packaging<24 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from mlflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from alembic<2->mlflow) (4.3.0)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Using cached PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Using cached websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from docker<7,>=4.0.0->mlflow) (1.26.12)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from Flask<3->mlflow) (2.2.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.9)\n",
      "Requirement already satisfied: setuptools>=3.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from gunicorn<21->mlflow) (63.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.37.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from shap<1,>=0.40->mlflow) (4.64.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Using cached slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Collecting numba\n",
      "  Downloading numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (613 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.7/613.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /home/mnsosa/miniconda3/envs/seiz_pred/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.17.6-py3-none-any.whl size=143221 sha256=f0df4eb5223d67913e7022d86ac6c6ded65fea4f080d007420b57fb5596a5695\n",
      "  Stored in directory: /home/mnsosa/.cache/pip/wheels/7b/85/a1/37b9b24f55c1da8ea4795765de41713b8a2cccad89c866e26b\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: websocket-client, tabulate, sqlparse, slicer, querystring-parser, pyjwt, Mako, llvmlite, itsdangerous, gunicorn, greenlet, cloudpickle, sqlalchemy, numba, Flask, docker, databricks-cli, shap, alembic, mlflow\n",
      "Successfully installed Flask-2.2.3 Mako-1.2.4 alembic-1.10.2 cloudpickle-2.2.1 databricks-cli-0.17.6 docker-6.0.1 greenlet-2.0.2 gunicorn-20.1.0 itsdangerous-2.1.2 llvmlite-0.39.1 mlflow-2.2.2 numba-0.56.4 pyjwt-2.6.0 querystring-parser-1.2.4 shap-0.41.0 slicer-0.0.7 sqlalchemy-2.0.7 sqlparse-0.4.3 tabulate-0.9.0 websocket-client-1.5.1\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataclass template for hyperparameters\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    \"\"\"Hyperparameters for the tensorflow model\"\"\"\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    debug: bool\n",
    "    filters: int\n",
    "    kernel_size: int\n",
    "    pool_size: int\n",
    "    dropout: float\n",
    "    dense_size: int\n",
    "    loss: str\n",
    "    optimizer: str\n",
    "    metrics: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyperparameters(epochs=10, batch_size=32, debug=True, filters=32, kernel_size=3, pool_size=2, dropout=0.5, dense_size=128, loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_baseline = Hyperparameters(\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    debug=True,\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    dropout=0.5,\n",
    "    dense_size=128,\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "hyperparameters_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate session: mlflow-ui\n"
     ]
    }
   ],
   "source": [
    "# Checking MLFlow\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "!tmux new-session -d -s mlflow-ui 'mlflow ui'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 19:35:15.320089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 19:35:16.304869: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2023-03-29 19:35:16.304938: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2023-03-29 19:35:16.306556: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "2023-03-29 19:35:16.661532: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sz_utils import data_handler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# check if gpu is available\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load training data\n",
    "\n",
    "preictal, interictal = data_handler.make_patient_windows(\"chb01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (672, 1280, 22)\n",
      "y_train: (672, 1)\n",
      "X_val: (84, 1280, 22)\n",
      "y_val: (84, 1)\n",
      "X_test: (84, 1280, 22)\n",
      "y_test: (84, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((preictal, interictal), axis=0)\n",
    "y = np.concatenate((np.ones((preictal.shape[0], 1)), np.zeros((interictal.shape[0], 1))), axis=0)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffle_indices = np.random.permutation(np.arange(X.shape[0]))\n",
    "X = X[shuffle_indices]\n",
    "y = y[shuffle_indices]\n",
    "\n",
    "# Split the data into train and test\n",
    "train_size = int(X.shape[0] * 0.8)\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "# Split test data into validation and test\n",
    "val_size = int(X_test.shape[0] * 0.5)\n",
    "X_val = X_test[:val_size]\n",
    "y_val = y_test[:val_size]\n",
    "X_test = X_test[val_size:]\n",
    "y_test = y_test[val_size:]\n",
    "\n",
    "# Shapes\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"y_val:\", y_val.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 1278, 256)         17152     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1278, 256)         0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 639, 256)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 163584)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                10469440  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,486,657\n",
      "Trainable params: 10,486,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# make model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(256, 3, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 5/21 [======>.......................] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.0180s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.0180s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 1s 32ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 10.6190 - val_accuracy: 0.9881\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm5uyxib_/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm5uyxib_/model/data/model/assets\n",
      "2023/03/29 20:00:44 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpm5uyxib_/model, flavor: tensorflow), fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "# track training with mlflow\n",
    "\n",
    "\n",
    "# create mlflow experiment\n",
    "mlflow.set_experiment(\"chb01\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"auto batch size\") as run:\n",
    "    mlflow.tensorflow.autolog()\n",
    "    model.fit(X_train, y_train, epochs=10, \n",
    "                        validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
