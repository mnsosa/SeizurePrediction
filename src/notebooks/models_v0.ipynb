{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from sz_utils import data_handler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# check if gpu is available\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# collect the data\n",
    "preictal, interictal = data_handler.make_patient_windows(\"chb01\")\n",
    "\n",
    "# make the labels\n",
    "X = np.concatenate((preictal, interictal), axis=0)\n",
    "y = np.concatenate((np.ones((preictal.shape[0], 1)), np.zeros((interictal.shape[0], 1))), axis=0)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffle_indices = np.random.permutation(np.arange(X.shape[0]))\n",
    "X = X[shuffle_indices]\n",
    "y = y[shuffle_indices]\n",
    "\n",
    "# Split the data into train and test\n",
    "train_size = int(X.shape[0] * 0.8)\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "# Split test data into validation and test\n",
    "val_size = int(X_test.shape[0] * 0.5)\n",
    "X_val = X_test[:val_size]\n",
    "y_val = y_test[:val_size]\n",
    "X_test = X_test[val_size:]\n",
    "y_test = y_test[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,LSTM, Conv1D, Activation, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (672, 1280, 22) y_train shape: (672, 1) X_val shape: (84, 1280, 22) y_val shape: (84, 1) X_test shape: (84, 1280, 22) y_test shape: (84, 1)\n"
     ]
    }
   ],
   "source": [
    "# shapes\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape, \"X_val shape:\", X_val.shape, \"y_val shape:\", y_val.shape, \"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 22)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 2 \n",
    "input_shape_dataset = (X_train.shape[1], X_train.shape[2])\n",
    "input_shape_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2 \n",
    "input_shape_dataset = (X_train.shape[1], X_train.shape[2])\n",
    "input_shape_dataset\n",
    "\n",
    "def create_model_cnn_basic_1_layer(\n",
    "    input_shape_dataset: tuple = input_shape_dataset,\n",
    "    num_classes: int = num_classes,\n",
    "    debug: bool = False,\n",
    "    filters: int = 256,\n",
    "    kernel_size: int = 3,\n",
    "    pool_size: int = 2,\n",
    "    dropout: float = 0.1,\n",
    "    dense_size: int = 64,\n",
    "    loss: str = \"binary_crossentropy\",\n",
    "    optimizer: str = \"adam\",\n",
    "    metrics: list = [\"accuracy\"],\n",
    "\n",
    ") -> tf.keras.Model:\n",
    "\n",
    "    \"\"\"This function creates a basic convolutional neural network model with 2 convolutional layers, 2 dense layers and a softmax layer\n",
    "\n",
    "    :param input_shape_dataset: shape of the input data\n",
    "    :type input_shape_dataset: tuple\n",
    "    :param num_classes: number of classes\n",
    "    :type num_classes: int\n",
    "    :return: return a model\n",
    "    :rtype: tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    if debug:\n",
    "        print(\"------------model summary---------------\")\n",
    "        print(\"input_shape_dataset\", input_shape_dataset)\n",
    "        print(\"num_classes\", num_classes)\n",
    "\n",
    "    input_shape_dataset: tuple\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters, kernel_size, input_shape=(input_shape_dataset)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_size))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "class Experimento:\n",
    "    experiment_name: str\n",
    "    model_name: str\n",
    "    model: tf.keras.Model\n",
    "    dataset: tuple\n",
    "    hyperparameters: dict\n",
    "    metrics: dict\n",
    "\n",
    "\n",
    "    def __init__(self, experiment_name, model_name, model, dataset, hyperparameters, metrics):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.metrics = metrics      \n",
    "\n",
    "    def fit(self):\n",
    "        # history = self.model.fit(  self.dataset[0],\n",
    "        #                             self.dataset[1],\n",
    "        #                             validation_data=(self.dataset[2], self.dataset[3]),\n",
    "        #                             epochs=self.hyperparameters[\"epochs\"],\n",
    "        #                         )\n",
    "        # return history\n",
    "\n",
    "        # with mlflow.start_run(nested=True):\n",
    "        mlflow.tensorflow.autolog()\n",
    "        history = self.model.fit(self.dataset[0], self.dataset[1],\n",
    "                                    validation_data=(self.dataset[2], self.dataset[3]),\n",
    "                                    epochs=self.hyperparameters[\"epochs\"],\n",
    "                                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n",
    "        # TODO log artifacts\n",
    "        # mlflow.log_artifacts(\"graphs\", self._log_graphs(history))\n",
    "        return history\n",
    "\n",
    "    def set_experiment(self):\n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "\n",
    "    def log_params(self):\n",
    "        mlflow.log_param(\"model_name\", self.model_name)\n",
    "        for key, value in self.hyperparameters.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "    \n",
    "    def log_metrics(self, history):\n",
    "        for metric_name, metric_values in history.history.items():\n",
    "            for epoch, value in enumerate(metric_values):\n",
    "                mlflow.log_metric(f\"{metric_name}\", value, step=epoch)\n",
    "    \n",
    "    # def _log_graphs(self, history):\n",
    "        # TODO make a function to log graphs\n",
    "    \n",
    "        # Save training and validation loss and accuracy plots as artifact\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8, 8), dpi=100, sharex=True)\n",
    "        # ax[0].plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "        # ax[0].plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "        # ax[0].set_title(\"Training/Validation Loss\")\n",
    "        # ax[0].set_xlabel(\"Epoch\")\n",
    "        # ax[0].set_ylabel(\"Loss\")\n",
    "        # # ax[0].set_grid(True)\n",
    "        # ax[0].legend()\n",
    "        \n",
    "        # ax[1].plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "        # ax[1].plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "        # ax[1].set_title(\"Training/Validation Accuracy\")\n",
    "        # ax[1].set_xlabel(\"Epoch\")\n",
    "        # ax[1].set_ylabel(\"Accuracy\")\n",
    "        # # ax[1].set_grid(True)\n",
    "        # ax[1].legend()\n",
    "        \n",
    "        # fig.tight_layout()\n",
    "        # fig.savefig(\"graphs.png\")\n",
    "        # plt.close(fig)\n",
    "        # return \"graphs.png\"\n",
    "        \n",
    "    def log_artifacts(self):\n",
    "        \n",
    "        pass\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze if it is necessary to use gc.collect()\n",
    "# import gc\n",
    "# gc.collect()\n",
    "def registrar_experimento(experimento: Experimento):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        experimento.set_experiment()\n",
    "        experimento.log_params()\n",
    "        experimento.log_metrics(experimento.fit())\n",
    "        # experimento._log_graphs(experimento.fit())\n",
    "        experimento.log_artifacts()\n",
    "\n",
    "# registrar_experimento(experimento_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimento_1 = Experimento(\n",
    "    experiment_name = \"CNN_autolog\",\n",
    "    model_name = \"CNN_basic_1_layer\",\n",
    "    model = create_model_cnn_basic_1_layer(),\n",
    "    dataset = (X_train, y_train, X_val, y_val),\n",
    "    hyperparameters = {\n",
    "        \"epochs\": 11,\n",
    "        \"filters\": 256,\n",
    "        \"kernel_size\": 3,\n",
    "        \"pool_size\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"dense_size\": 64,\n",
    "        \"loss\": \"binary_crossentropy\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \n",
    "    },metrics = [\"accuracy\"],\n",
    ")\n",
    "\n",
    "experimento_2 = Experimento(\n",
    "    experiment_name = \"CNN_autolog\",\n",
    "    model_name = \"CNN_basic_1_layer\",\n",
    "    model = create_model_cnn_basic_1_layer(),\n",
    "    dataset = (X_train, y_train, X_val, y_val),\n",
    "    hyperparameters = {\n",
    "        \"epochs\": 17,\n",
    "        \"filters\": 256,\n",
    "        \"kernel_size\": 3,\n",
    "        \"pool_size\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"dense_size\": 32,\n",
    "        \"loss\": \"binary_crossentropy\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \n",
    "    },metrics = [\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0400s). Check your callbacks.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\crist\\AppData\\Local\\Temp\\tmpvujkpdge\\model\\data\\model\\assets\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_train_batch_end` time: 0.0184s). Check your callbacks.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\crist\\AppData\\Local\\Temp\\tmp_on0sdbz\\model\\data\\model\\assets\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "experimentos = [experimento_1, experimento_2]\n",
    "\n",
    "for experimento in experimentos:\n",
    "    registrar_experimento(experimento)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
